{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c10c088",
   "metadata": {},
   "source": [
    "# Exploring sentiment analysis on Yelp data\n",
    "\n",
    "In this notebook, I tried to perform sentiment analysis on the Yelp hotel reviews dataset using 4 techniques. \n",
    "\n",
    "* TextBlob\n",
    "* VADAR\n",
    "* Using a pre-trained distillBERT model from HuggingFace's transformers\n",
    "* Training my own NN with a transformer block and embedding layer on Yelp data with TensorFlow.\n",
    "\n",
    "I also did some experiments with training a Naive Bayes classifier in TextBlob, but there is few documentation on that technique online but I left the codes in here for posterity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "be0b7e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import string\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "from textblob import TextBlob\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "import tensorflow \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "14a7100b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date                reviewID              reviewerID  \\\n",
      "0   6/8/2011            MyNjnxzZVTPq  IFTr6_6NI4CgCVavIL9k5g   \n",
      "1  8/30/2011  BdD7fsPqHQL73hwENEDT-Q  c_-hF15XgNhlyy_TqzmdaA   \n",
      "2  6/26/2009                BfhqiyfC  CiwZ6S5ZizAFL5gypf8tLA   \n",
      "3  9/16/2010                      Ol  nf3q2h-kSQoZK2jBY92FOg   \n",
      "4   2/5/2010  i4HIAcNTjabdpG1K4F5Q2g  Sb3DJGdZ4Rq__CqxPbae-g   \n",
      "\n",
      "                                       reviewContent  rating  usefulCount  \\\n",
      "0  Let me begin by saying that there are two kind...       5           18   \n",
      "1  The only place inside the Loop that you can st...       3            0   \n",
      "2  I have walked by the Tokyo Hotel countless tim...       5           12   \n",
      "3  If you are considering staying here, watch thi...       1            8   \n",
      "4  This place is disgusting, absolutely horrible,...       3           11   \n",
      "\n",
      "   coolCount  funnyCount flagged                 hotelID  \n",
      "0         11          28       N  tQfLGoolUMu2J0igcWcoZg  \n",
      "1          3           4       N  tQfLGoolUMu2J0igcWcoZg  \n",
      "2         14          23       N  tQfLGoolUMu2J0igcWcoZg  \n",
      "3          2           6       N  tQfLGoolUMu2J0igcWcoZg  \n",
      "4          4           9       N  tQfLGoolUMu2J0igcWcoZg  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read sqlite query results into a pandas DataFrame\n",
    "con = sqlite3.connect(\"yelpHotelData.db\")\n",
    "df = pd.read_sql_query(\"SELECT * from review\", con)\n",
    "\n",
    "# Verify that result of SQL query is stored in the dataframe\n",
    "print(df.head())\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06f0ca1",
   "metadata": {},
   "source": [
    "# Explore the Yelp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b2937197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 688329 entries, 0 to 688328\n",
      "Data columns (total 10 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   date           688329 non-null  object\n",
      " 1   reviewID       688329 non-null  object\n",
      " 2   reviewerID     688329 non-null  object\n",
      " 3   reviewContent  688329 non-null  object\n",
      " 4   rating         688329 non-null  int64 \n",
      " 5   usefulCount    688329 non-null  int64 \n",
      " 6   coolCount      688329 non-null  int64 \n",
      " 7   funnyCount     688329 non-null  int64 \n",
      " 8   flagged        688329 non-null  object\n",
      " 9   hotelID        688329 non-null  object\n",
      "dtypes: int64(4), object(6)\n",
      "memory usage: 52.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# Get info \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1b9609bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove funny characters\n",
    "df['reviewContent'] = df['reviewContent'].apply(lambda x: str(x).replace(u'\\xa0', u''))\n",
    "df['reviewContent'] = df['reviewContent'].apply(lambda x: str(x).strip(\"/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fd72a5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped by here to meet a friend last night after a long day at the office.  I had no idea that there was a place with such character, around all the characterless places in the area!  All I could think was that I was continuing my fine tradition of being in a dive bar, dressed for business, where my open toe shoes could scoop up the peanut shells on the floor! What can I say? An interesting mix of people from blue collar to white collar to some guy from out of town with his shirt half off and crooning along with the jukebox which I must say had some pretty fine songs!  A little bit of Monk's trivia is that all the light fixtures have been collected from antique stores around the world -which is why they don't match adding to the eclectic feel of the place. Monk's has a large beer selection and while I didn't have any food- my friend's burger looked and smelled great and came with curly fries. Where do you get those anymore these days? It was like an 80's flashback! Another plus? The bartender was amazingly adept in keeping the shirtless man from totally ruining my evening (or maybe she was just keeping me from giving the guy a swift kick! Hmm . . .) Whatever the case- Monk's gave me the first laugh of my day and that was at 9pm! Go for the \"kitschyness\", go to meet friends, go for the laughs, go for the drinks and food. But if you are that guy without the shirt last night- I suggest you go somewhere else as I won't miss the next time!\n"
     ]
    }
   ],
   "source": [
    "# Print to check\n",
    "print(df.reviewContent[99999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cad5960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label for the outcome. \n",
    "conditions = [\n",
    "    (df['rating'] < 3),\n",
    "    (df['rating'] >=  3)]\n",
    "values = ['neg', 'pos']\n",
    "\n",
    "# Create label column\n",
    "df['label'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8fbfcc",
   "metadata": {},
   "source": [
    "Before examining the dataset more, I want to split it into a training and test set. I also want a validation set to test the model on to tune my parameters before touching the test set. I decided to count 3/5 as positive because businesses would care more about the truly disastrous cases that resulted in 1 & 2 stars reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ed85fe90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "688329"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "577aabe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------PARTITION INFO---------\n",
      "train_data shape: (8000, 11)\n",
      "val_data shape: (1500, 11)\n",
      "test_data shape: (500, 11)\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "\n",
    "# Shuffle the data first\n",
    "df = shuffle(df,random_state=34)\n",
    "\n",
    "# Reduce the data set to 10000 reviews\n",
    "df = df[:10000]\n",
    "\n",
    "# 80% training, 15% validation, 5% test\n",
    "CUT1=int(0.80*df.shape[0])\n",
    "CUT2=int((0.80+0.15)*df.shape[0])\n",
    "\n",
    "\n",
    "train_data, val_data, test_data= df[:CUT1], df[CUT1:CUT2], df[CUT2:]\n",
    "print('------PARTITION INFO---------')\n",
    "print(\"train_data shape:\",train_data.shape)\n",
    "print(\"val_data shape:\"  ,val_data.shape)\n",
    "print(\"test_data shape:\" ,test_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dcc945",
   "metadata": {},
   "source": [
    "# Explore the distribution of the rating in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "771eb0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAVNUlEQVR4nO3df4zc9X3n8ec7hqMWGwyR0z3X9p0tna86gxUar3yuENVug4ovoDOVGskRF6BN5R6ip0RFOkz+uKSqLPmPI60IgZN7jmwEycpSwtkiuC2lrFAlKLUp7WJcGqusqH/IVgMxbA5xsvPuH/PlNCyzOzPf2ZlZ+Dwf0mhmPt/vZz7v72fGL8985zvfjcxEklSGTwy7AEnS4Bj6klQQQ1+SCmLoS1JBDH1JKshlwy6gnZUrV+a6detq9f3pT3/KlVdeubgFLQLr6o51dce6uvNxrevYsWP/nJmf/tCCzFzSl82bN2ddzz77bO2+/WRd3bGu7lhXdz6udQFHs0WmuntHkgpi6EtSQQx9SSqIoS9JBTH0Jakghr4kFcTQl6SCGPqSVBBDX5IKsuRPwyBp6Vi364e1+9676SJ31ew/s+eW2uPqg3ynL0kFMfQlqSCGviQVxNCXpIK0Df2I+LmIeDEi/jYijkfE71ftn4qIpyPiR9X1NU197o+IkxHxWkTc3NS+OSKmq2UPRkT0Z7MkSa108k7/PeBXM/MzwPXAtojYCuwCnsnMDcAz1X0iYiOwA7gW2AY8HBHLqsd6BNgJbKgu2xZvUyRJ7bQN/ep8/LPV3curSwLbgQNV+wHgtur2dmAyM9/LzNeBk8CWiFgFXJWZz1cn+H+0qY8kaQCikb9tVmq8Uz8G/Dvg25l5X0T8JDOvblrnrcy8JiIeAl7IzMeq9n3AEWAG2JOZN1XtNwL3ZeatLcbbSeMTAaOjo5snJydrbdzs7CwjIyO1+vaTdXXHurrTz7qmT1+o3Xd0OZx7t17fTatX1B63nY/r8zgxMXEsM8fmtnf046zMvARcHxFXA09ExHULrN5qP30u0N5qvL3AXoCxsbEcHx/vpMwPmZqaom7ffrKu7lhXd/pZV90fV0Hjx1kPTNf7PejM7eO1x22ntOexq6N3MvMnwBSNffHnql02VNfnq9VOAWubuq0BzlTta1q0S5IGpJOjdz5dvcMnIpYDNwF/DxwG7qxWuxM4VN0+DOyIiCsiYj2NL2xfzMyzwDsRsbU6aueOpj6SpAHo5LPWKuBAtV//E8DBzHwyIp4HDkbEl4E3gC8AZObxiDgIvApcBO6pdg8B3A3sB5bT2M9/ZDE3RpK0sLahn5l/B/xSi/YfA5+bp89uYHeL9qPAQt8HSJL6yF/kSlJBDH1JKoihL0kFMfQlqSCGviQVxNCXpIIY+pJUEENfkgpi6EtSQQx9SSqIoS9JBTH0Jakghr4kFcTQl6SCGPqSVBBDX5IKYuhLUkEMfUkqiKEvSQUx9CWpIIa+JBXE0Jekghj6klQQQ1+SCtI29CNibUQ8GxEnIuJ4RHylav9GRJyOiJery+eb+twfEScj4rWIuLmpfXNETFfLHoyI6M9mSZJauayDdS4C92bmSxHxSeBYRDxdLfvDzPyfzStHxEZgB3At8AvAn0fEv8/MS8AjwE7gBeApYBtwZHE2RZLUTtt3+pl5NjNfqm6/A5wAVi/QZTswmZnvZebrwElgS0SsAq7KzOczM4FHgdt63QBJUueikb8drhyxDngOuA74PeAu4G3gKI1PA29FxEPAC5n5WNVnH4138zPAnsy8qWq/EbgvM29tMc5OGp8IGB0d3Tw5OVlr42ZnZxkZGanVt5+sqzvW1Z1+1jV9+kLtvqPL4dy79fpuWr2i9rjtfFyfx4mJiWOZOTa3vZPdOwBExAjwfeCrmfl2RDwC/AGQ1fUDwG8BrfbT5wLtH27M3AvsBRgbG8vx8fFOy/yAqakp6vbtJ+vqjnV1p5913bXrh7X73rvpIg9Mdxw5HzBz+3jtcdsp7Xns6OidiLicRuA/npk/AMjMc5l5KTN/BvwxsKVa/RSwtqn7GuBM1b6mRbskaUA6OXongH3Aicz8ZlP7qqbVfh14pbp9GNgREVdExHpgA/BiZp4F3omIrdVj3gEcWqTtkCR1oJPPWjcAXwKmI+Llqu1rwBcj4noau2hmgN8ByMzjEXEQeJXGkT/3VEfuANwN7AeW09jP75E7kjRAbUM/M/+S1vvjn1qgz25gd4v2ozS+BJYkDYG/yJWkghj6klQQQ1+SCmLoS1JBDH1JKoihL0kFMfQlqSCGviQVxNCXpIIY+pJUEENfkgpi6EtSQQx9SSqIoS9JBTH0Jakghr4kFcTQl6SCGPqSVBBDX5IKYuhLUkEMfUkqiKEvSQUx9CWpIIa+JBWkbehHxNqIeDYiTkTE8Yj4StX+qYh4OiJ+VF1f09Tn/og4GRGvRcTNTe2bI2K6WvZgRER/NkuS1Eon7/QvAvdm5n8AtgL3RMRGYBfwTGZuAJ6p7lMt2wFcC2wDHo6IZdVjPQLsBDZUl22LuC2SpDbahn5mns3Ml6rb7wAngNXAduBAtdoB4Lbq9nZgMjPfy8zXgZPAlohYBVyVmc9nZgKPNvWRJA1ANPK3w5Uj1gHPAdcBb2Tm1U3L3srMayLiIeCFzHysat8HHAFmgD2ZeVPVfiNwX2be2mKcnTQ+ETA6Orp5cnKy1sbNzs4yMjJSq28/WVd3rKs7/axr+vSF2n1Hl8O5d+v13bR6Re1x2/m4Po8TExPHMnNsbvtlnT5ARIwA3we+mplvL7A7vtWCXKD9w42Ze4G9AGNjYzk+Pt5pmR8wNTVF3b79ZF3dsa7u9LOuu3b9sHbfezdd5IHpjiPnA2ZuH689bjvt5mtdD9vci/3bRvryPHZ09E5EXE4j8B/PzB9UzeeqXTZU1+er9lPA2qbua4AzVfuaFu2SpAHp5OidAPYBJzLzm02LDgN3VrfvBA41te+IiCsiYj2NL2xfzMyzwDsRsbV6zDua+kiSBqCTz1o3AF8CpiPi5arta8Ae4GBEfBl4A/gCQGYej4iDwKs0jvy5JzMvVf3uBvYDy2ns5z+yOJshSepE29DPzL+k9f54gM/N02c3sLtF+1EaXwJLkobAX+RKUkEMfUkqiKEvSQUx9CWpIIa+JBXE0Jekghj6klQQQ1+SClLv7EeSmD59oacTkPViZs8tQxlXH32+05ekghj6klQQQ1+SCmLoS1JBDH1JKoihL0kFMfQlqSCGviQVxNCXpIIY+pJUEENfkgpi6EtSQQx9SSqIoS9JBTH0JakgbUM/Ir4TEecj4pWmtm9ExOmIeLm6fL5p2f0RcTIiXouIm5vaN0fEdLXswYiIxd8cSdJCOnmnvx/Y1qL9DzPz+uryFEBEbAR2ANdWfR6OiGXV+o8AO4EN1aXVY0qS+qht6Gfmc8CbHT7edmAyM9/LzNeBk8CWiFgFXJWZz2dmAo8Ct9WsWZJUUzQyuM1KEeuAJzPzuur+N4C7gLeBo8C9mflWRDwEvJCZj1Xr7QOOADPAnsy8qWq/EbgvM2+dZ7ydND4VMDo6unlycrLWxs3OzjIyMlKrbz9ZV3eWal3n37zAuXeHM/am1SvmXdbP+Zo+faF239Hl1J6vhba3V+3mq5dt7sX6Fct6eh4nJiaOZebY3Pa6fyP3EeAPgKyuHwB+C2i1nz4XaG8pM/cCewHGxsZyfHy8VpFTU1PU7dtP1tWdpVrXtx4/xAPTw/kz0zO3j8+7rJ/z1cvfBL5308Xa87XQ9vaq3XwN6+8g7992ZV+ex1pH72Tmucy8lJk/A/4Y2FItOgWsbVp1DXCmal/Tol2SNEC1Qr/aR/++XwfeP7LnMLAjIq6IiPU0vrB9MTPPAu9ExNbqqJ07gEM91C1JqqHtZ62I+B4wDqyMiFPA14HxiLiexi6aGeB3ADLzeEQcBF4FLgL3ZOal6qHupnEk0HIa+/mPLOJ2SJI60Db0M/OLLZr3LbD+bmB3i/ajwHVdVSdJWlT+IleSCmLoS1JBDH1JKoihL0kFMfQlqSCGviQVxNCXpIIY+pJUEENfkgpi6EtSQQx9SSqIoS9JBTH0Jakghr4kFcTQl6SCGPqSVBBDX5IKYuhLUkEMfUkqiKEvSQUx9CWpIIa+JBXE0Jekghj6klSQtqEfEd+JiPMR8UpT26ci4umI+FF1fU3Tsvsj4mREvBYRNze1b46I6WrZgxERi785kqSFdPJOfz+wbU7bLuCZzNwAPFPdJyI2AjuAa6s+D0fEsqrPI8BOYEN1mfuYkqQ+axv6mfkc8Oac5u3Ager2AeC2pvbJzHwvM18HTgJbImIVcFVmPp+ZCTza1EeSNCDRyOA2K0WsA57MzOuq+z/JzKublr+VmddExEPAC5n5WNW+DzgCzAB7MvOmqv1G4L7MvHWe8XbS+FTA6Ojo5snJyVobNzs7y8jISK2+/WRd3VmqdZ1/8wLn3h3O2JtWr5h3WT/na/r0hdp9R5dTe74W2t5etZuvXra5F+tXLOvpeZyYmDiWmWNz2y/rqaoPa7WfPhdobykz9wJ7AcbGxnJ8fLxWMVNTU9Tt20/W1Z2lWte3Hj/EA9OL/U+oMzO3j8+7rJ/zddeuH9bue++mi7Xna6Ht7VW7+eplm3uxf9uVfXke6x69c67aZUN1fb5qPwWsbVpvDXCmal/Tol2SNEB1Q/8wcGd1+07gUFP7joi4IiLW0/jC9sXMPAu8ExFbq6N27mjqI0kakLaftSLie8A4sDIiTgFfB/YAByPiy8AbwBcAMvN4RBwEXgUuAvdk5qXqoe6mcSTQchr7+Y8s6pZIktpqG/qZ+cV5Fn1unvV3A7tbtB8FruuqOknSovIXuZJUEENfkgpi6EtSQQx9SSqIoS9JBTH0Jakghr4kFcTQl6SCGPqSVBBDX5IKYuhLUkEMfUkqiKEvSQUZzp/9Ud+s6/EvG9X9K0Eze26pPa6kwfGdviQVxNCXpIIY+pJUEENfkgpi6EtSQQx9SSqIoS9JBTH0Jakghr4kFcTQl6SC9BT6ETETEdMR8XJEHK3aPhURT0fEj6rra5rWvz8iTkbEaxFxc6/FS5K6sxjv9Ccy8/rMHKvu7wKeycwNwDPVfSJiI7ADuBbYBjwcEcsWYXxJUof6sXtnO3Cgun0AuK2pfTIz38vM14GTwJY+jC9JmkevoZ/An0XEsYjYWbWNZuZZgOr656v21cA/NfU9VbVJkgYkMrN+54hfyMwzEfHzwNPAfwMOZ+bVTeu8lZnXRMS3gecz87GqfR/wVGZ+v8Xj7gR2AoyOjm6enJysVd/s7CwjIyO1+vZTP+uaPn2hdt/R5XDu3Xp9N61eUXvcdpbq83j+zQu156tXC823r6/utJuvXra5F+tXLOvpeZyYmDjWtNv9/+vpfPqZeaa6Ph8RT9DYXXMuIlZl5tmIWAWcr1Y/Baxt6r4GODPP4+4F9gKMjY3l+Ph4rfqmpqao27ef+llX3fPhQ+N8+g9M13tJzNw+Xnvcdpbq8/itxw/Vnq9eLTTfvr66026+etnmXuzfdmVfnsfau3ci4sqI+OT7t4FfA14BDgN3VqvdCRyqbh8GdkTEFRGxHtgAvFh3fElS93p5mzIKPBER7z/OdzPzTyLir4GDEfFl4A3gCwCZeTwiDgKvAheBezLzUk/VS5K6Ujv0M/Mfgc+0aP8x8Ll5+uwGdtcdU5LUG3+RK0kFMfQlqSDDOfRgQKZPXxjKN+8ze24Z+JiS1Anf6UtSQQx9SSqIoS9JBTH0Jakghr4kFcTQl6SCGPqSVBBDX5IKYuhLUkEMfUkqiKEvSQUx9CWpIIa+JBXE0Jekghj6klQQQ1+SCmLoS1JBDH1JKoihL0kFMfQlqSCGviQVxNCXpIIMPPQjYltEvBYRJyNi16DHl6SSDTT0I2IZ8G3gPwEbgS9GxMZB1iBJJRv0O/0twMnM/MfM/H/AJLB9wDVIUrEiMwc3WMRvANsy87er+18C/mNm/u6c9XYCO6u7vwi8VnPIlcA/1+zbT9bVHevqjnV15+Na17/NzE/PbbyshwesI1q0feh/nczcC+ztebCIo5k51uvjLDbr6o51dce6ulNaXYPevXMKWNt0fw1wZsA1SFKxBh36fw1siIj1EfGvgB3A4QHXIEnFGujuncy8GBG/C/wpsAz4TmYe7+OQPe8i6hPr6o51dce6ulNUXQP9IleSNFz+IleSCmLoS1JBPvKhHxHfiYjzEfHKPMsjIh6sTvvwdxHx2SVS13hEXIiIl6vL/xhQXWsj4tmIOBERxyPiKy3WGficdVjXwOcsIn4uIl6MiL+t6vr9FusMY746qWsor7Fq7GUR8TcR8WSLZUP5N9lBXcP6NzkTEdPVmEdbLF/c+crMj/QF+BXgs8Ar8yz/PHCExm8EtgJ/tUTqGgeeHMJ8rQI+W93+JPAPwMZhz1mHdQ18zqo5GKluXw78FbB1CcxXJ3UN5TVWjf17wHdbjT+sf5Md1DWsf5MzwMoFli/qfH3k3+ln5nPAmwussh14NBteAK6OiFVLoK6hyMyzmflSdfsd4ASwes5qA5+zDusauGoOZqu7l1eXuUc/DGO+OqlrKCJiDXAL8L/nWWUo/yY7qGupWtT5+siHfgdWA//UdP8USyBMKr9cfTw/EhHXDnrwiFgH/BKNd4nNhjpnC9QFQ5izapfAy8B54OnMXBLz1UFdMJzX2B8B/x342TzLh/X6+iMWrguGM18J/FlEHIvGKWjmWtT5KiH0Ozr1wxC8ROPcGJ8BvgX8n0EOHhEjwPeBr2bm23MXt+gykDlrU9dQ5iwzL2Xm9TR+Qb4lIq6bs8pQ5quDugY+XxFxK3A+M48ttFqLtr7OV4d1Devf5A2Z+VkaZx++JyJ+Zc7yRZ2vEkJ/SZ76ITPffv/jeWY+BVweESsHMXZEXE4jWB/PzB+0WGUoc9aurmHOWTXmT4ApYNucRUN9jc1X15Dm6wbgP0fEDI2z6P5qRDw2Z51hzFfbuob1+srMM9X1eeAJGmcjbrao81VC6B8G7qi+Ad8KXMjMs8MuKiL+dUREdXsLjefixwMYN4B9wInM/OY8qw18zjqpaxhzFhGfjoirq9vLgZuAv5+z2jDmq21dw5ivzLw/M9dk5joap1n5i8z8L3NWG/h8dVLXkF5fV0bEJ9+/DfwaMPeIv0Wdr0GfZXPRRcT3aHzrvjIiTgFfp/GlFpn5v4CnaHz7fRL4v8BvLpG6fgO4OyIuAu8CO7L6qr7PbgC+BExX+4MBvgb8m6bahjFnndQ1jDlbBRyIxh8A+gRwMDOfjIj/2lTXMOark7qG9Rr7kCUwX53UNYz5GgWeqP6vuQz4bmb+ST/ny9MwSFJBSti9I0mqGPqSVBBDX5IKYuhLUkEMfUkqiKEvSQUx9CWpIP8CcvBXZWiM5dMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data.rating.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6541c515",
   "metadata": {},
   "source": [
    "From the training data, we could see that reviews skew positive. We will use 3 as the cut off. We will only consider 2 classes: positive (rating>=3) and negative. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b76ce",
   "metadata": {},
   "source": [
    "# Make a custom classifier using TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e272ae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use the randomized index to split the df\n",
    "# # Using this tutorial https://textblob.readthedocs.io/en/dev/classifiers.html\n",
    "\n",
    "# train = list(zip(train_data.reviewContent,train_data.label))\n",
    "# val = list(zip(val_data.reviewContent,val_data.label))\n",
    "# test = list(zip(test_data.reviewContent,test_data.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f612689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example of what the train set looks like\n",
    "# train[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5668960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a Naive Bayes classifier, passing the training data into the constructor.\n",
    "# cl = NaiveBayesClassifier(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c48e7486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the classififer on the training set\n",
    "# cl.accuracy(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18502e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the classififer on the validation set\n",
    "# cl.accuracy(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a86cf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate it on the test set\n",
    "# cl.accuracy(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811522b3",
   "metadata": {},
   "source": [
    "<!-- The customized classififer does equally well on the validation set and the test set.\n",
    " -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "244f3626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show the most informative features\n",
    "# cl.show_informative_features(5)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3c24cb",
   "metadata": {},
   "source": [
    "# TextBlob Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d083e8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get sentiment score from TextBlob\n",
    "# https://stackoverflow.com/questions/43485469/apply-textblob-in-for-each-row-of-a-dataframe\n",
    "def sentiment_calc(text):\n",
    "    try:\n",
    "        return TextBlob(text).sentiment.polarity\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0e49d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to assign polarity label\n",
    "def polarity_label(polarity_value):\n",
    "    if polarity_value < 0:\n",
    "        lab = 'neg'     \n",
    "    if polarity_value >= 0:\n",
    "        lab = 'pos'\n",
    "    return lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f873ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the sentiment_calc function on the training set\n",
    "train_data['blob_polarity'] = train_data.reviewContent.apply(sentiment_calc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25c817ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['blob_label'] = train_data.blob_polarity.apply(lambda x: polarity_label(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "88ef339f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewContent</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>blob_polarity</th>\n",
       "      <th>blob_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>396284</th>\n",
       "      <td>Went here with my nieces and wife on a thursda...</td>\n",
       "      <td>4</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.407500</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48305</th>\n",
       "      <td>This glossy and glassy skyscraper designed by ...</td>\n",
       "      <td>3</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640273</th>\n",
       "      <td>Came here for a Union League event and this pl...</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.292036</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385327</th>\n",
       "      <td>I love going to dinner and being able to try a...</td>\n",
       "      <td>4</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.515789</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425149</th>\n",
       "      <td>This Ralph's had a decent meat and seafood cou...</td>\n",
       "      <td>2</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.163542</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8938</th>\n",
       "      <td>I am so proud to be adding this business! My f...</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.397516</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645399</th>\n",
       "      <td>Girl date with my Carrots, and it was decreed ...</td>\n",
       "      <td>4</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.196339</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323300</th>\n",
       "      <td>The food was good, but the service was extreme...</td>\n",
       "      <td>3</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.090064</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534126</th>\n",
       "      <td>Since we were in the area, Ciara G and I decid...</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.338818</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513676</th>\n",
       "      <td>Nestled in an Asian shopping center near my ap...</td>\n",
       "      <td>3</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.153472</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636438</th>\n",
       "      <td>Yum, yum, yum! This spanking brand new bakery ...</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.354669</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614868</th>\n",
       "      <td>For some reason all the Home Depots in the poo...</td>\n",
       "      <td>1</td>\n",
       "      <td>neg</td>\n",
       "      <td>-0.684524</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492716</th>\n",
       "      <td>My favorite place in the Wynn! the atmosphere ...</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.562619</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228961</th>\n",
       "      <td>So this is definitely one of those touristy sp...</td>\n",
       "      <td>4</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404114</th>\n",
       "      <td>Got in this time, and it's still good. The dou...</td>\n",
       "      <td>4</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354791</th>\n",
       "      <td>Local brunch fixture with all the classics and...</td>\n",
       "      <td>4</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.106818</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313087</th>\n",
       "      <td>Convenient location if you work downtown or an...</td>\n",
       "      <td>4</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.057720</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136736</th>\n",
       "      <td>Feh! to the Joshua Tree. I feel as though it's...</td>\n",
       "      <td>2</td>\n",
       "      <td>neg</td>\n",
       "      <td>-0.056280</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>My elderly parents needed an ADA compliant roo...</td>\n",
       "      <td>3</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.328022</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685994</th>\n",
       "      <td>The two ladies here made my day. They sounded ...</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.294080</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            reviewContent  rating label  \\\n",
       "396284  Went here with my nieces and wife on a thursda...       4   pos   \n",
       "48305   This glossy and glassy skyscraper designed by ...       3   pos   \n",
       "640273  Came here for a Union League event and this pl...       5   pos   \n",
       "385327  I love going to dinner and being able to try a...       4   pos   \n",
       "425149  This Ralph's had a decent meat and seafood cou...       2   neg   \n",
       "8938    I am so proud to be adding this business! My f...       5   pos   \n",
       "645399  Girl date with my Carrots, and it was decreed ...       4   pos   \n",
       "323300  The food was good, but the service was extreme...       3   pos   \n",
       "534126  Since we were in the area, Ciara G and I decid...       5   pos   \n",
       "513676  Nestled in an Asian shopping center near my ap...       3   pos   \n",
       "636438  Yum, yum, yum! This spanking brand new bakery ...       5   pos   \n",
       "614868  For some reason all the Home Depots in the poo...       1   neg   \n",
       "492716  My favorite place in the Wynn! the atmosphere ...       5   pos   \n",
       "228961  So this is definitely one of those touristy sp...       4   pos   \n",
       "404114  Got in this time, and it's still good. The dou...       4   pos   \n",
       "354791  Local brunch fixture with all the classics and...       4   pos   \n",
       "313087  Convenient location if you work downtown or an...       4   pos   \n",
       "136736  Feh! to the Joshua Tree. I feel as though it's...       2   neg   \n",
       "1432    My elderly parents needed an ADA compliant roo...       3   pos   \n",
       "685994  The two ladies here made my day. They sounded ...       5   pos   \n",
       "\n",
       "        blob_polarity blob_label  \n",
       "396284       0.407500        pos  \n",
       "48305        0.050000        pos  \n",
       "640273       0.292036        pos  \n",
       "385327       0.515789        pos  \n",
       "425149       0.163542        pos  \n",
       "8938         0.397516        pos  \n",
       "645399       0.196339        pos  \n",
       "323300       0.090064        pos  \n",
       "534126       0.338818        pos  \n",
       "513676       0.153472        pos  \n",
       "636438       0.354669        pos  \n",
       "614868      -0.684524        neg  \n",
       "492716       0.562619        pos  \n",
       "228961       0.325000        pos  \n",
       "404114       0.350000        pos  \n",
       "354791       0.106818        pos  \n",
       "313087       0.057720        pos  \n",
       "136736      -0.056280        neg  \n",
       "1432         0.328022        pos  \n",
       "685994       0.294080        pos  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out a few examples\n",
    "train_data[['reviewContent','rating','label','blob_polarity','blob_label']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d90f305",
   "metadata": {},
   "source": [
    "As we expected, because the reviews tend to skew positive, TextBlob gives us overwhelming positive results as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b138beed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.54      0.31      0.40      1104\n",
      "         pos       0.90      0.96      0.93      6896\n",
      "\n",
      "    accuracy                           0.87      8000\n",
      "   macro avg       0.72      0.64      0.66      8000\n",
      "weighted avg       0.85      0.87      0.85      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(train_data.label, train_data.blob_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "ad5eb34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.61      0.33      0.43        70\n",
      "         pos       0.90      0.97      0.93       430\n",
      "\n",
      "    accuracy                           0.88       500\n",
      "   macro avg       0.75      0.65      0.68       500\n",
      "weighted avg       0.86      0.88      0.86       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply the sentiment_calc function on the test set\n",
    "test_data['blob_polarity'] = test_data.reviewContent.apply(sentiment_calc)\n",
    "# Apply the polarity_label function:\n",
    "test_data['blob_label'] = test_data['blob_polarity'].apply(lambda x: polarity_label(x))\n",
    "\n",
    "print(classification_report(test_data.label, test_data.blob_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a1a66f",
   "metadata": {},
   "source": [
    "Out of the box, the accuracy of TextBlob is 68% on the training set and 70% on the validation set. A business would care more about the negative reviews. In this case, the high precision and low recall score for the negative label (-1) is a good thing. Because we have an imbalanced set where more reviews are positive, the model predicted fewer negative reviews, but most of its predicted negative reviews are correct when compared to the training labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ef1c40",
   "metadata": {},
   "source": [
    "# VADAR sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "02096f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to use VADAR sentiment\n",
    "# https://gist.github.com/BenjaminFraser/fc4dd29549a75c93336822060d012ec8\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "def vadar_sentiment(text):\n",
    "    \"\"\" Calculate and return the nltk vadar (lexicon method) sentiment \"\"\"\n",
    "    return sia.polarity_scores(text)['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "80a8fcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply vadar_sentiment on training data\n",
    "train_data['vadar_polarity'] = train_data['reviewContent'].apply(vadar_sentiment)\n",
    "# Apply polarity function\n",
    "train_data['vadar_label'] = train_data.vadar_polarity.apply(lambda x: polarity_label(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "020c46b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewContent</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>vadar_polarity</th>\n",
       "      <th>vadar_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>396284</th>\n",
       "      <td>Went here with my nieces and wife on a thursda...</td>\n",
       "      <td>4</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.9577</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48305</th>\n",
       "      <td>This glossy and glassy skyscraper designed by ...</td>\n",
       "      <td>3</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.8360</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640273</th>\n",
       "      <td>Came here for a Union League event and this pl...</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.9757</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385327</th>\n",
       "      <td>I love going to dinner and being able to try a...</td>\n",
       "      <td>4</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.9935</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425149</th>\n",
       "      <td>This Ralph's had a decent meat and seafood cou...</td>\n",
       "      <td>2</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.8699</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8938</th>\n",
       "      <td>I am so proud to be adding this business! My f...</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.9982</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645399</th>\n",
       "      <td>Girl date with my Carrots, and it was decreed ...</td>\n",
       "      <td>4</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.9281</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323300</th>\n",
       "      <td>The food was good, but the service was extreme...</td>\n",
       "      <td>3</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.8074</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534126</th>\n",
       "      <td>Since we were in the area, Ciara G and I decid...</td>\n",
       "      <td>5</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513676</th>\n",
       "      <td>Nestled in an Asian shopping center near my ap...</td>\n",
       "      <td>3</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.5167</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            reviewContent  rating label  \\\n",
       "396284  Went here with my nieces and wife on a thursda...       4   pos   \n",
       "48305   This glossy and glassy skyscraper designed by ...       3   pos   \n",
       "640273  Came here for a Union League event and this pl...       5   pos   \n",
       "385327  I love going to dinner and being able to try a...       4   pos   \n",
       "425149  This Ralph's had a decent meat and seafood cou...       2   neg   \n",
       "8938    I am so proud to be adding this business! My f...       5   pos   \n",
       "645399  Girl date with my Carrots, and it was decreed ...       4   pos   \n",
       "323300  The food was good, but the service was extreme...       3   pos   \n",
       "534126  Since we were in the area, Ciara G and I decid...       5   pos   \n",
       "513676  Nestled in an Asian shopping center near my ap...       3   pos   \n",
       "\n",
       "        vadar_polarity vadar_label  \n",
       "396284          0.9577         pos  \n",
       "48305           0.8360         pos  \n",
       "640273          0.9757         pos  \n",
       "385327          0.9935         pos  \n",
       "425149          0.8699         pos  \n",
       "8938            0.9982         pos  \n",
       "645399          0.9281         pos  \n",
       "323300          0.8074         pos  \n",
       "534126          0.9979         pos  \n",
       "513676          0.5167         pos  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out a few examples\n",
    "train_data[['reviewContent','rating','label','vadar_polarity','vadar_label']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "27e5ad19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply vadar_sentiment on training data\n",
    "test_data['vadar_polarity'] = test_data['reviewContent'].apply(vadar_sentiment)\n",
    "# Apply polarity function\n",
    "test_data['vadar_label'] = test_data.vadar_polarity.apply(lambda x: polarity_label(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "95b554d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.64      0.33      0.43        70\n",
      "         pos       0.90      0.97      0.93       430\n",
      "\n",
      "    accuracy                           0.88       500\n",
      "   macro avg       0.77      0.65      0.68       500\n",
      "weighted avg       0.86      0.88      0.86       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_data.label, test_data.vadar_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9fbafc",
   "metadata": {},
   "source": [
    "# Tranformers - Hugging Face distilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "edbc9022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializer pipeline\n",
    "cls = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "               max_length=512, truncation =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e0fa4a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['bert_label'] = train_data.reviewContent.apply(lambda x: cls(x)[0]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "985c5d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relabel(x):\n",
    "    # Create label for the outcome. \n",
    "    conditions = [\n",
    "        (x['bert_label']  == 'POSITIVE'),\n",
    "        (x['bert_label'] ==  'NEGATIVE')]\n",
    "    values = ['pos', 'neg']\n",
    "\n",
    "    # Create label column\n",
    "    x['bert_label'] = np.select(conditions, values)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "3cddfd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = relabel(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "10321d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.40      0.87      0.55      1106\n",
      "         pos       0.97      0.79      0.87      6894\n",
      "\n",
      "    accuracy                           0.80      8000\n",
      "   macro avg       0.69      0.83      0.71      8000\n",
      "weighted avg       0.89      0.80      0.83      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(train_data.label, train_data.bert_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a985af",
   "metadata": {},
   "source": [
    "# Train my own NN with transformer block using keras on the Yelp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "6fba68e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/examples/nlp/text_classification_with_transformer/\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e9fb5792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "b0c5f9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81539994",
   "metadata": {},
   "source": [
    "# Redo the train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "f17b0180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37282 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# This came from the book Deep Learning with Python by F. Chollett\n",
    "\n",
    "maxlen = 200  # We will cut reviews after 100 words\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df['reviewContent'])\n",
    "sequences = tokenizer.texts_to_sequences(df['reviewContent'])\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "b1f9f68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo the train test split\n",
    "x_train, x_val, x_test= data[:CUT1], data[CUT1:CUT2], data[CUT2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "9e97c049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import label encoder\n",
    "from sklearn import preprocessing\n",
    " \n",
    "# label_encoder object knows how to understand word labels.\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    " \n",
    "# Encode outcome labels in column \n",
    "df['label']= label_encoder.fit_transform(df['label'])\n",
    "\n",
    "\n",
    "labels = np.asarray(df.label)\n",
    "y_train, y_val, y_test = labels[:CUT1], labels[CUT1:CUT2], labels[CUT2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "979d6c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/examples/nlp/text_classification_with_transformer/\n",
    "\n",
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "e95b0be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "250/250 [==============================] - 11s 41ms/step - loss: 0.0097 - accuracy: 0.9975 - val_loss: 0.9441 - val_accuracy: 0.8740\n",
      "Epoch 2/50\n",
      "250/250 [==============================] - 11s 43ms/step - loss: 0.0044 - accuracy: 0.9985 - val_loss: 1.0846 - val_accuracy: 0.8640\n",
      "Epoch 3/50\n",
      "250/250 [==============================] - 11s 42ms/step - loss: 0.0045 - accuracy: 0.9984 - val_loss: 1.0892 - val_accuracy: 0.8773\n",
      "Epoch 4/50\n",
      "250/250 [==============================] - 11s 42ms/step - loss: 0.0073 - accuracy: 0.9977 - val_loss: 1.0602 - val_accuracy: 0.8507\n",
      "Epoch 5/50\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 0.0035 - accuracy: 0.9987 - val_loss: 1.2456 - val_accuracy: 0.8487\n",
      "Epoch 6/50\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 4.4544e-04 - accuracy: 0.9999 - val_loss: 1.2480 - val_accuracy: 0.8740\n",
      "Epoch 7/50\n",
      "250/250 [==============================] - 10s 42ms/step - loss: 6.1452e-05 - accuracy: 1.0000 - val_loss: 1.3419 - val_accuracy: 0.8700\n",
      "Epoch 8/50\n",
      "250/250 [==============================] - 10s 42ms/step - loss: 1.8623e-05 - accuracy: 1.0000 - val_loss: 1.3858 - val_accuracy: 0.8713\n",
      "Epoch 9/50\n",
      "250/250 [==============================] - 11s 43ms/step - loss: 1.5879e-05 - accuracy: 1.0000 - val_loss: 1.4256 - val_accuracy: 0.8713\n",
      "Epoch 10/50\n",
      "250/250 [==============================] - 11s 42ms/step - loss: 1.0851e-05 - accuracy: 1.0000 - val_loss: 1.4645 - val_accuracy: 0.8713\n",
      "Epoch 11/50\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 1.2071e-05 - accuracy: 1.0000 - val_loss: 1.4995 - val_accuracy: 0.8707\n",
      "Epoch 12/50\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 6.8171e-06 - accuracy: 1.0000 - val_loss: 1.5277 - val_accuracy: 0.8713\n",
      "Epoch 13/50\n",
      "250/250 [==============================] - 10s 42ms/step - loss: 6.1324e-06 - accuracy: 1.0000 - val_loss: 1.5560 - val_accuracy: 0.8713\n",
      "Epoch 14/50\n",
      "250/250 [==============================] - 11s 43ms/step - loss: 9.1212e-06 - accuracy: 1.0000 - val_loss: 1.5894 - val_accuracy: 0.8713\n",
      "Epoch 15/50\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 5.0861e-06 - accuracy: 1.0000 - val_loss: 1.6144 - val_accuracy: 0.8713\n",
      "Epoch 16/50\n",
      "250/250 [==============================] - 10s 42ms/step - loss: 4.9064e-06 - accuracy: 1.0000 - val_loss: 1.6430 - val_accuracy: 0.8700\n",
      "Epoch 17/50\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 3.5064e-06 - accuracy: 1.0000 - val_loss: 1.6692 - val_accuracy: 0.8707\n",
      "Epoch 18/50\n",
      "250/250 [==============================] - 11s 43ms/step - loss: 2.6206e-06 - accuracy: 1.0000 - val_loss: 1.6938 - val_accuracy: 0.8707\n",
      "Epoch 19/50\n",
      "250/250 [==============================] - 11s 43ms/step - loss: 3.0240e-06 - accuracy: 1.0000 - val_loss: 1.7226 - val_accuracy: 0.8707\n",
      "Epoch 20/50\n",
      "250/250 [==============================] - 10s 42ms/step - loss: 3.2127e-06 - accuracy: 1.0000 - val_loss: 1.7526 - val_accuracy: 0.8693\n",
      "Epoch 21/50\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 1.9058e-06 - accuracy: 1.0000 - val_loss: 1.7784 - val_accuracy: 0.8680\n",
      "Epoch 22/50\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 3.5474e-06 - accuracy: 1.0000 - val_loss: 1.8157 - val_accuracy: 0.8680\n",
      "Epoch 23/50\n",
      "250/250 [==============================] - 11s 42ms/step - loss: 1.5978e-06 - accuracy: 1.0000 - val_loss: 1.8374 - val_accuracy: 0.8687\n",
      "Epoch 24/50\n",
      "250/250 [==============================] - 11s 43ms/step - loss: 1.1500e-06 - accuracy: 1.0000 - val_loss: 1.8581 - val_accuracy: 0.8680\n",
      "Epoch 25/50\n",
      "250/250 [==============================] - 10s 40ms/step - loss: 9.1276e-07 - accuracy: 1.0000 - val_loss: 1.8792 - val_accuracy: 0.8680\n",
      "Epoch 26/50\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 1.7974e-06 - accuracy: 1.0000 - val_loss: 1.9135 - val_accuracy: 0.8680\n",
      "Epoch 27/50\n",
      "250/250 [==============================] - 11s 43ms/step - loss: 9.0407e-07 - accuracy: 1.0000 - val_loss: 1.9366 - val_accuracy: 0.8680\n",
      "Epoch 28/50\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 6.3879e-07 - accuracy: 1.0000 - val_loss: 1.9577 - val_accuracy: 0.8680\n",
      "Epoch 29/50\n",
      "250/250 [==============================] - 10s 42ms/step - loss: 9.8904e-07 - accuracy: 1.0000 - val_loss: 1.9916 - val_accuracy: 0.8687\n",
      "Epoch 30/50\n",
      "250/250 [==============================] - 11s 44ms/step - loss: 9.3478e-07 - accuracy: 1.0000 - val_loss: 2.0223 - val_accuracy: 0.8680\n",
      "Epoch 31/50\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 3.6779e-07 - accuracy: 1.0000 - val_loss: 2.0424 - val_accuracy: 0.8673\n",
      "Epoch 32/50\n",
      "250/250 [==============================] - 11s 42ms/step - loss: 2.9430e-07 - accuracy: 1.0000 - val_loss: 2.0611 - val_accuracy: 0.8667\n",
      "Epoch 33/50\n",
      "250/250 [==============================] - 11s 43ms/step - loss: 8.0510e-07 - accuracy: 1.0000 - val_loss: 2.1002 - val_accuracy: 0.8673\n",
      "Epoch 34/50\n",
      "250/250 [==============================] - 11s 42ms/step - loss: 2.7130e-07 - accuracy: 1.0000 - val_loss: 2.1214 - val_accuracy: 0.8673\n",
      "Epoch 35/50\n",
      "250/250 [==============================] - 11s 43ms/step - loss: 1.6921e-07 - accuracy: 1.0000 - val_loss: 2.1365 - val_accuracy: 0.8673\n",
      "Epoch 36/50\n",
      "250/250 [==============================] - 11s 42ms/step - loss: 3.2562e-07 - accuracy: 1.0000 - val_loss: 2.1672 - val_accuracy: 0.8673\n",
      "Epoch 37/50\n",
      "250/250 [==============================] - 11s 44ms/step - loss: 2.5310e-07 - accuracy: 1.0000 - val_loss: 2.1934 - val_accuracy: 0.8673\n",
      "Epoch 38/50\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 2.3317e-07 - accuracy: 1.0000 - val_loss: 2.2230 - val_accuracy: 0.8673\n",
      "Epoch 39/50\n",
      "250/250 [==============================] - 11s 42ms/step - loss: 1.1748e-07 - accuracy: 1.0000 - val_loss: 2.2401 - val_accuracy: 0.8673\n",
      "Epoch 40/50\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 1.4300e-07 - accuracy: 1.0000 - val_loss: 2.2672 - val_accuracy: 0.8673\n",
      "Epoch 41/50\n",
      "250/250 [==============================] - 11s 43ms/step - loss: 1.0396e-07 - accuracy: 1.0000 - val_loss: 2.2878 - val_accuracy: 0.8667\n",
      "Epoch 42/50\n",
      "250/250 [==============================] - 11s 42ms/step - loss: 9.8197e-08 - accuracy: 1.0000 - val_loss: 2.3088 - val_accuracy: 0.8673\n",
      "Epoch 43/50\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 1.3674e-07 - accuracy: 1.0000 - val_loss: 2.3401 - val_accuracy: 0.8667\n",
      "Epoch 44/50\n",
      "250/250 [==============================] - 11s 42ms/step - loss: 7.0840e-08 - accuracy: 1.0000 - val_loss: 2.3636 - val_accuracy: 0.8673\n",
      "Epoch 45/50\n",
      "250/250 [==============================] - 11s 42ms/step - loss: 1.7885e-07 - accuracy: 1.0000 - val_loss: 2.4061 - val_accuracy: 0.8680\n",
      "Epoch 46/50\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 1.1801e-07 - accuracy: 1.0000 - val_loss: 2.4442 - val_accuracy: 0.8680\n",
      "Epoch 47/50\n",
      "250/250 [==============================] - 11s 43ms/step - loss: 7.4620e-08 - accuracy: 1.0000 - val_loss: 2.4723 - val_accuracy: 0.8687\n",
      "Epoch 48/50\n",
      "250/250 [==============================] - 10s 42ms/step - loss: 2.3797e-08 - accuracy: 1.0000 - val_loss: 2.4867 - val_accuracy: 0.8687\n",
      "Epoch 49/50\n",
      "250/250 [==============================] - 11s 42ms/step - loss: 5.0990e-08 - accuracy: 1.0000 - val_loss: 2.5152 - val_accuracy: 0.8687\n",
      "Epoch 50/50\n",
      "250/250 [==============================] - 10s 41ms/step - loss: 2.4140e-08 - accuracy: 1.0000 - val_loss: 2.5338 - val_accuracy: 0.8687\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    x_train, y_train, batch_size=32, epochs=50, validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "c0dee3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 16ms/step - loss: 2.5777 - accuracy: 0.8640\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.5777125358581543, 0.8640000224113464]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "6b6f959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "predictions = model.predict(x_test)\n",
    "# Convert probabilities to classes\n",
    "y_pred = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "0d5417e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.39      0.44        70\n",
      "           1       0.90      0.94      0.92       430\n",
      "\n",
      "    accuracy                           0.86       500\n",
      "   macro avg       0.71      0.66      0.68       500\n",
      "weighted avg       0.85      0.86      0.86       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6e680f",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "\n",
    "I tried 4 different methods. Reviews tend to skew positive and businesses ideally should focus on correctly identifying the true negatives in reviews so they can identify problems within their operations. Because we're dealing with a classification problem ('positive' or 'negative' review) with imbalanced classes (fewer negative reviews than positive reviews), we should focus on precision & recall when comparing the models. Below are the precision/ recall metrics for the **negative** class for all 4 approaches on the same test set of ~500 reviews\n",
    "\n",
    "* TextBlob: precision: 0.61  recal: 0.33\n",
    "* VADAR: precision 0.64 recall: 0.33 \n",
    "* Using a pre-trained distillBERT: precision: 0.40 recall: 0.87\n",
    "* training my own NN with a transformer block on Yelp data in using TensorFlow: precision: 0.52 recall: 0.39\n",
    "\n",
    "From these metrics, using the pre-trained distillBERT seems to be the way to go - the high recall rates means that the model returned a low false negative rate - which means that the negatives that it did pick out were actually negatives. The drawback of distilBERT is that it took my computer (I have a brand new Macbook Pro with the new Apple Silicon M1 Max chip & 10 CPU cores) quite a while to run, but I can see how powerful this technique can be for businesses and policymakers who do not have the capital/ resources to go through each and every review to identify negative customer feedback."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
